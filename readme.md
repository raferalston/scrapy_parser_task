### Как использовать
- py -m venv env
- env\Scripts\activate или source env\bin\activate
- pip install requirements.txt

### Тесты
- cd hh_task_project
- pytest

### Запуск парсилки
- scrapy crawl goods_parser -O result.json

### FAQ
- В settings.py указать нужные урлы в переменной START_URLS
- По api собираем данные, поэтому город указывается в виде uuid в переменной CITIES_UUID
- Прокси вкл/выкл в переменной PROXY

# Scrapy Parser Task

Парсер на базе [Scrapy](https://scrapy.org/) для извлечения структурированных данных с целевого сайта.

## Цель проекта

- Автоматически обходить страницы выбранного сайта  
- Извлекать интересующие данные (название, цену, рейтинг, описание и т.п.)  
- Сохранять результат в удобном формате (CSV / JSON / XML)

## Функциональность

- Конфигурируемый `settings.py` — URL-ы, задержки, количество одновременных запросов  
- Обработчик пагинации, переход к следующим страницам  
- Фильтрация и валидация извлекаемых данных  
- Экспорт в `data/output.csv` и/или `data/output.json`

